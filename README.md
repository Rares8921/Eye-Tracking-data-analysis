**Summary**

I manually watched every video using Neon Player, noted the exact start and stop times for each Waldo level, and used gimp to find the exact pixel coordinates of Waldo. Once this data was established, I ran the scripts in a specific order to process the data, followed by the yolo notebook.

**Folder & File Legend**

* recordings/ & neon_player_fixations/ (Input): This contains the raw data folders exported directly from the eye tracker hardware. Each subfolder represents one participant session and contains csv files with raw data and the reference video `world.mp4`.

* dataset/ (Input): The dataset/ directory serves as the main source of all training and evaluation data. It contains the level images and labels as well.

        wget https://storage.googleapis.com/public-file-server/waldo_training_images.tar
        wget https://storage.googleapis.com/public-file-server/waldo_validation_images.tar

* waldo_fixations/ (Generated Output): Generated by `waldo_fixations.py`. This folder contains the filtered results from the first script. Instead of keeping the entire recording, these files only store the specific moments where the participant actually looked at Waldo.

* features/ (Generated Output): Generated by `features_build.py`. This folder holds the calculated behavioral profiles for each recording. Instead of lists of coordinates, these csv files contain data like saccade length, revisits, gaze entropy etc.

* attention_maps/ (Generated Output): Generated by `data_build.py`. This folder contains the visual results generated by `data_build.py`. It holds `.npy` files (raw numerical maps) and `.png` images (heatmaps overlaid on the game board) showing exactly where participants looked.

* saliency_metrics/ (Generated Output): Generated by `compute_saliency_metrics.py`. It holds files comparing the human heatmaps against mathematical algorithms that predict where people *should* look.

* metrics_outputs/ (Generated Output): Generated by `metrics_summary.py`. Aggregates everything into summary tables and final polished overlay images for the report.

**Script Legend (+ run order)**

1. waldo_fixations.py
    
    The raw data coming from the Eye Tracker is just a large stream of coordinates. The sensor doesn't know when Level 1 started or where Waldo actually is on the screen.
    <br/>
    <br/>
* I took the timestamps I manually noted down (from watching the replays in Neon Player) and hardcoded them into a dictionary. This dictionary maps each recording ID to the exact start and end seconds of every level. Similarly, I defined another dictionary containing the normalized bounding boxes (x, y) for Waldo in each level.
    <br/>
    <br/>
    * The script iterates through these specific time slices and filters the gaze data. Crucially, I didn't just look for perfect hits. I implemented a logic that classifies a look as direct if it falls exactly inside Waldo's box, or peripheral if it falls within a specific margin around him. This is important because it captures the moments when the participant spotted Waldo without staring dead-center at him.
    <br/>
    <br/>
    * It generates the csv files containing only the relevant fixations.
    <br/>
    <br/>
2. features_build.py

    Turn raw coordinates into meaningful statistics about *how* I searched and *how efficient* I was. Raw fixation points tell me nothing about my confusion level or my search speed. This script acts as the statistical engine of the project.
    <br/>
    <br/>
* This script reads the general fixation data (from the **recordings/** folder) and the specific Waldo hit data (from the **waldo_fixations/** folder) to calculate complex behavioral metrics.
    <br/>
    <br/>
    * Calculated Metrics Include: I compute the Saccade Length Average (a proxy for search speed), Fixation Revisits (a measure of confusion/inefficiency), and Gaze Entropy (quantifying the randomness of the search pattern). I also calculate various fixation ratios and duration averages.
    <br/>
    <br/>
    * The script generates individual csv files for each recording and a final summary table (`features_summary.csv`) containing the complete behavioral profile for all participants, ready for comparative analysis.
    <br/>
    <br/>
3. data_build.py

    The goal here is to visually understand spatial attention and identify distractors. We need to see the data visually, not just as numbers, to pinpoint where people got stuck.
    <br/>
    <br/>
* This script converts the normalized coordinates into visual Heatmaps (kde and gaussian). The purpose is to see the density of attention across the entire map.
    <br/>
    <br/>
    * Crucially, it runs a clustering algorithm (DBSCAN) on the raw gaze data to find Distractors—specific spots on the map that repeatedly tricked people but were not Waldo. This identifies human confusion.
    <br/>
    <br/>
    * It generates the visual maps (.png and .npy files) and cluster statistics for each recording.
    <br/>
    <br/>
4. compute_saliency_metrics.py

    This script forms the scientific core: comparing human decisions against what mathematical models predict. We need to scientifically prove how human vision differs from computer vision.
    <br/>
    <br/>
* The core of the script is loading or generating the Saliency Map—the computer's guess of where people should look, based purely on image contrast and color.
    <br/>
    <br/>
    * It compares the human fixations against this Saliency Map using standard academic metrics: NSS and AUC-Judd. These metrics determine if the human was guided by basic visual stimuli (bottom-up) or by complex strategies (top-down).
    <br/>
    <br/>
    * It outputs a final summary csv table containing all metric scores for the analysis.
    <br/>
    <br/>
5. metrics_summary.py

    The purpose of this script is simple: too many output files were generated by the pipeline. We have hundreds of small result files scattered around.
    <br/>
    <br/>
* This script loops through all the outputs from steps 1-4 (metrics, fixations, saliency data) and combines them into one single csv file (`metrics_summary.csv`).
    <br/>
    <br/>
    * It also generates the final high-quality visual overlays needed for the presentation, ensuring the heatmaps look smooth and professional.
    <br/>
    <br/>
    * It generates the final, combined csv report and the visual overlays.
    <br/>
    <br/>
6.  Yolo Standard Training.ipynb

    Finding waldo using yolo
    <br/>
    <br/>
* As the test images' quality is bad, I added some preprocessing to highlight Waldo’s stripes. The core is the retry mechanism: First, it scans quickly at standard resolution. If it fails, it tries the other branch.
    <br/>
    <br/>
    * This involves re-loading the image at high resolution, lowering the confidence threshold, and using augmentation (zooming/rotating) trying to mimic a human’s deep scan.
    <br/>
    <br/>
    * It performs the final detection and displays the result on screen.
    <br/>
    <br/>

**Some assumptions from recordings so far**
* Learning Effect: I observed a significant decrease in search time after the first level. Participants quickly mentally calibrated to Waldo’s specific pattern and color palette after the initial exposure.
* No Return Bias: Participants tend to consciously avoid looking at the spot where Waldo was located in previous images, assuming his position is randomized.

**What is Left to Do (Todo)**

* Try to recompute saliency with a better model
* Integrate recording features into yolo
* Analyze confusion and predictivity
* Generate gaze path recordings of type human vs ai to check if users figure out who is who
